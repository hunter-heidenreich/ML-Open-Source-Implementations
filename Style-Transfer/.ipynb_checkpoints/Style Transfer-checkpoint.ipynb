{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Tutorial\n",
    "Style transfer is, hands down, one of my favorite things. I think the first time it was explained to me was early on when I was starting to learn about machine learning and deep learning. I was hooked. Today, we are going to walk through the style transfer script that I adapted and talk about some of what is going on with it. By the end of it, we should be able to get something like this:\n",
    "![Example city generation](styletransfer.gif \"City generated by this script\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Okay cool! Now that we know what we are aiming for, let's dive in. To get started, we are going to need the following import statements to work, so go ahead and download the packages if you don't already have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from PIL import Image # This is actually installed using `pip install Pillow` \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So let's talk a bit about what's going on here before we dive in. At a very high level, the way that style transfer works is a little something like this:\n",
    "\n",
    "- Select an image that you want to preserve the content of (a person, a city, a landscape, etc.)\n",
    "- Select an image that you want to transfer the style of (a painting or something like that)\n",
    "- Take a pre-trained convolutional NN (we'll be using VGG16) and get the outputs at certain layers to gain feature representations of these images at specific layers\n",
    "- Generate a random noise image of the same size\n",
    "- Create loss functions that represent the difference of the noise and the content, and the noise and the style\n",
    "- Minimize the loss between the two for optimal agreement\n",
    "- Output your image!\n",
    "\n",
    "Okay, so that doesn't seem too bad. \n",
    "\n",
    "For this tutorial, we'll be using this image as our content image:\n",
    "![Philly scene](images/city.png)\n",
    "\n",
    "And this for our style image:\n",
    "![](images/style1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Loss\n",
    "Let's start off by talking about content loss and how we define it. It's fairly simple actually!\n",
    "\n",
    "If you think about a CNN, the base layers start with low-level representations. Maybe lines, then parts, then entire entitities. It builds from the bottom up. If you have a CNN that recognizes faces, maybe it starts with borders first, and learns higher representations like noses, eyes, and mouths in subsequent layers. The final CNN layers are what actually recognize faces.\n",
    "\n",
    "We'll use that same logic to define our content loss. We'll take one of the final CNN layers of the VGG16 model and we'll grab that for our content loss. How does our loss function actually look? Here is how the equation is defined:\n",
    "![Content loss equation](http://ankitmathur.me/classes/final_files/image02.jpg)\n",
    "\n",
    "Looks menacing! But worry not. It reads like this:\n",
    "- We have a function that we call content loss \n",
    "- That function takes in 3 parameters\n",
    "    - p -> the content target\n",
    "    - x -> the generated image\n",
    "    - l -> the layer in question \n",
    "- We construct 2 feature representations with this information\n",
    "    - F -> the feature representation of the generated image\n",
    "        - In English, this is what the layer outputs to us when we run our image through the network\n",
    "    - P -> the feature representation of the content target \n",
    "- We take the element-wise difference (subtract each index value by the matching one between the two matrices)\n",
    "- We take the element-wise square of this (square each index value)\n",
    "- We sum all those values together \n",
    "- We divide by 2 (or multiply by 0.5)\n",
    "\n",
    "And that's our loss function! We'll actually define our code to take in two feature representations and just do the operations because its simpler that way. Here's how we'll write the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content_features, generated_features):\n",
    "    \"\"\"\n",
    "    Computes the content loss\n",
    "    :param content_features: The features of the content image\n",
    "    :param generated_features: The features of the generated image\n",
    "    :return: The content loss\n",
    "    \"\"\"\n",
    "    return 0.5 * K.sum(K.square(generated_features - content_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "So the style loss is a little more involved than the content loss. This because while content is pretty easy to define (just what is the subject of the image), style is not quite so obvious. What we'll do is use something called the gram matrix. It's a sort of correlation between aspect of the image fed through variou layers that allows us to numerically represent some features of what we naturally call style. \n",
    "\n",
    "The computation for that is defined as this:\n",
    "![Gram matrix calculation](http://ankitmathur.me/classes/final_files/image04.jpg)\n",
    "This is simply the dot product between the feature matrix and its transpose. Not too bad.\n",
    "\n",
    "Next the original paper gives us these two equations to define the loss:\n",
    "\n",
    "![](http://ankitmathur.me/classes/final_files/image05.jpg)\n",
    "![](http://ankitmathur.me/classes/final_files/image06.jpg)\n",
    "\n",
    "We'll come back to the the weighting aspect later on, but the first equation is essentially taking an L2 norm (a Frobenius norm).\n",
    "- Subtract element-wise the gram matrices\n",
    "- Square element-wise\n",
    "- Sum them together\n",
    "- Multiply by our factor 1/(4 * (color channels)^2 * (total pixel values)^2)\n",
    "\n",
    "And then we multiply by weighting based on layers, but we'll return to that in a bit. We'll actually use that first equation to define our style loss.\n",
    "\n",
    "** Note that the img_channels and img_size are global parameters that we'll actually define later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(features):\n",
    "    \"\"\"\n",
    "    Calculates the gram matrix of the feature representation matrix\n",
    "    :param features: The feature matrix that is used to calculate the gram matrix\n",
    "    :return: The gram matrix\n",
    "    \"\"\"\n",
    "    return K.dot(features, K.transpose(features))\n",
    "\n",
    "\n",
    "def style_loss(style_matrix, generated_matrix):\n",
    "    \"\"\"\n",
    "    Computes the style loss of the transfer\n",
    "    :param style_matrix: The style representation from the target style image\n",
    "    :param generated_matrix: The style representation from the generated image\n",
    "    :return: The loss from the style content\n",
    "    \"\"\"\n",
    "    # Permute the matrix to calculate proper covariance\n",
    "    style_features = K.batch_flatten(K.permute_dimensions(style_matrix, (2, 0, 1)))\n",
    "    generated_features = K.batch_flatten(K.permute_dimensions(generated_matrix, (2, 0, 1)))\n",
    "\n",
    "    # Get the gram matrices\n",
    "    style_mat = gram_matrix(style_features)\n",
    "    generated_mat = gram_matrix(generated_features)\n",
    "\n",
    "    return K.sum(K.square(style_mat - generated_mat)) / (4.0 * (img_channels ** 2) * (img_size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation Loss\n",
    "Now we'll talk about a term called variation loss. Simply speaking, it's a normalization term. It encourages smoothness and discourages noise. This leads to cleaner images that aren't as pixelated. No fancy math here, just shifting some pixel values so we'll drop the code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variation_loss(generated_matrix):\n",
    "    \"\"\"\n",
    "    Computes the variation loss metric (used for normalization)\n",
    "    :param generated_matrix: The generated matrix\n",
    "    :return: The variation loss term for normalization\n",
    "    \"\"\"\n",
    "    a = K.square(generated_matrix[:, :img_height-1, :img_width-1, :] - generated_matrix[:, 1:, :img_width-1, :])\n",
    "    b = K.square(generated_matrix[:, :img_height-1, :img_width-1, :] - generated_matrix[:, :img_height-1, 1:, :])\n",
    "\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss\n",
    "So now it's time to merge all the loss functions together. We'll assign each of them a weight that determines how much each will influence the overall cost. Assigning heavier loss to the style will result in more style, whereas heavier content loss will result in the output being more true to the original content.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(c_layer, s_layers, generated):\n",
    "    \"\"\"\n",
    "    Computes the total loss of a given iteration\n",
    "    :param c_layer: The layer used to compute the content loss\n",
    "    :param s_layers: The layer(s) used to compute the style loss\n",
    "    :param generated: The generated image\n",
    "    :return: The total loss\n",
    "    \"\"\"\n",
    "\n",
    "    content_weight = args.content_weight\n",
    "    style_weight = args.style_weight\n",
    "    variation_weight = args.var_weight\n",
    "\n",
    "    # Content loss\n",
    "    content_features = c_layer[CONTENT_IMAGE_POS, :, :, :]\n",
    "    generated_features = c_layer[GENERATED_IMAGE_POS, :, :, :]\n",
    "    c_loss = content_loss(content_features, generated_features)\n",
    "\n",
    "    # Style loss\n",
    "    s_loss = None\n",
    "    for layer in s_layers:\n",
    "        style_features = layer[STYLE_IMAGE_POS, :, :, :]\n",
    "        generated_features = layer[GENERATED_IMAGE_POS, :, :, :]\n",
    "        if s_loss is None:\n",
    "            s_loss = style_loss(style_features, generated_features) * (style_weight / len(s_layers))\n",
    "        else:\n",
    "            s_loss += style_loss(style_features, generated_features) * (style_weight / len(s_layers))\n",
    "\n",
    "    # Variation loss (for regularization)\n",
    "    v_loss = variation_loss(generated)\n",
    "\n",
    "    return content_weight * c_loss + s_loss + variation_weight * v_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is a function that is passed 3 variables. \n",
    "- c_layer -> The layer that determines content loss\n",
    "- s_layers -> The list of layers that factor into the style loss \n",
    "- generated -> The current generated image \n",
    "\n",
    "The arg parameters will be something we talk about towards the end, but these are basically calls to set the weights we wish to see given to these various loss factors. \n",
    "\n",
    "Content loss just gets the content for the two features and calculates the content loss. Style loss actually iterates through the layers, and multiplies by the weighting per layer here. Finally we just get the variation loss and multiply the 3 loss factors together using proper weighting. \n",
    "\n",
    "That's the value we return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Layers from the Model\n",
    "So we already mentioned that we are using the VGG16 model. You can read about the model [here](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3) but the TL;DR of it is that it's a 16 layer model that was used in ILSVRC comptetitons. We download a pre-trained version so that we can get straight into style transfer.\n",
    "\n",
    "We'll write a quick function to extract what we want from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(content_matrix, style_matrix, generated_matrix):\n",
    "    \"\"\"\n",
    "    Returns the content and style layers we need for the transfer\n",
    "    :param content_matrix: The feature matrix of the content image\n",
    "    :param style_matrix:  The feature matrix of the style image\n",
    "    :param generated_matrix:  The feature matrix of the generated image\n",
    "    :return: A tuple of content layers and style layers\n",
    "    \"\"\"\n",
    "    # Prep the model for our new input sizes\n",
    "    input_tensor = K.concatenate([content_matrix, style_matrix, generated_matrix], axis=0)\n",
    "    model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "\n",
    "    # Convert layers to dictionary\n",
    "    layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "    # Pull the specific layers we want\n",
    "    c_layers = layers['block2_conv2']\n",
    "    s_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "    s_layers = [layers[layer] for layer in s_layers]\n",
    "\n",
    "    return c_layers, s_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing in our 3 matrix representations we can get the model we need. We concatenate our matrices to make 1 input tensor so that we can quickly get our values later. You'll see above how we access those specific matrices through our global variables like 'STYLE_IMAGE_POS.' \n",
    "\n",
    "After building our input tensor, we grab our model. We give it our input tensor, preparing it for our specific input size. We ask for the imagenet weights. We also scrap the top layers. The top layers are the layers that involve flattening the network so that we can create a couple dense layers that output classification. We don't need these here, we only need layers involving convolutions, so we toss those top layers away.\n",
    "\n",
    "We can do a quick dict conversion to get our layers in a nice access fashion. Then we simply just pull the layers we want, and return a tuple of the layers back to the calling function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Talk Some Args and Globals\n",
    "\n",
    "So just be warned, if you're viewing this in the python notebook form, the parser and the arguments are not going to work... It'd be better to set these to globals and just modify the code where you see some of those pop up.\n",
    "\n",
    "But anways, most of this is self-explanatory. We are setting the arguments needed to do our style transfer. We make note of the position of our matrices in our input tensor for later usage as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Image neural style transfer implemented with Keras')\n",
    "parser.add_argument('content_img', metavar='content', type=str, help='Path to target content image')\n",
    "parser.add_argument('style_img', metavar='style', type=str, help='Path to target style image')\n",
    "parser.add_argument('result_img_prefix', metavar='res_prefix', type=str, help='Name of generated image')\n",
    "parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run')\n",
    "parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight')\n",
    "parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight')\n",
    "parser.add_argument('--var_weight', type=float, default=1.0, required=False, help='Total Variation weight')\n",
    "parser.add_argument('--height', type=int, default=512, required=False, help='Height of the images')\n",
    "parser.add_argument('--width', type=int, default=512, required=False, help='Width of the images')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Params #\n",
    "\n",
    "img_height = args.height\n",
    "img_width = args.width\n",
    "img_size = img_height * img_width\n",
    "img_channels = 3\n",
    "\n",
    "content_path = args.content_img\n",
    "style_path = args.style_img\n",
    "target_path = args.result_img_prefix\n",
    "target_extension = '.png'\n",
    "\n",
    "CONTENT_IMAGE_POS = 0\n",
    "STYLE_IMAGE_POS = 1\n",
    "GENERATED_IMAGE_POS = 2\n",
    "\n",
    "# Params #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Our Images\n",
    "So the natural format of our images aren't going to work. We need to modify them so that they play nice with our network. Here's the function we'll use to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(path):\n",
    "    \"\"\"\n",
    "    Function for processing images to the format we need\n",
    "    :param path: The path to the image\n",
    "    :return: The image as a data array, scaled and reflected\n",
    "    \"\"\"\n",
    "    # Open image and resize it\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((img_width, img_height))\n",
    "\n",
    "    # Convert image to data array\n",
    "    data = np.asarray(img, dtype='float32')\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "    data = data[:, :, :, :3]\n",
    "\n",
    "    # Apply pre-process to match VGG16 we are using\n",
    "    data[:, :, :, 0] -= 103.939\n",
    "    data[:, :, :, 1] -= 116.779\n",
    "    data[:, :, :, 2] -= 123.68\n",
    "\n",
    "    # Flip from RGB to BGR\n",
    "    data = data[:, :, :, ::-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We open the image using PIL and resize it\n",
    "- Then we conert it to a numpy array and expand it so that it follows the format we need\n",
    "    - The cutting off of the alpha channel is specifically done here, if you're using PNGs\n",
    "- We then pre-process the data so that it matches the means that VGG16 is expecting in terms of RGB values that it's been trained on \n",
    "- VGG16 is also expecting the data to be in the format of BGR instead of RGB so we do that flip here as well\n",
    "\n",
    "All this prepares our data so that we can build out our 3 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the generated image\n",
    "generated_img = np.random.uniform(0, 255, (1, img_height, img_width, 3)) - 128.\n",
    "\n",
    "# Load the respective content and style images\n",
    "content = process_img(content_path)\n",
    "style = process_img(style_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Variables\n",
    "Like mentioned in the beginning, this is just an optimization problem. In order to make this work, we'll define some variables using our above images. \n",
    "\n",
    "We'll also go ahead and grab our layers and define our loss and gradients. This will allow us to run our optimizations when we go through our iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the variables for the flow graph\n",
    "content_image = K.variable(content)\n",
    "style_image = K.variable(style)\n",
    "generated_image = K.placeholder((1, img_height, img_width, 3))\n",
    "loss = K.variable(0.)\n",
    "\n",
    "# Grab the layers needed to prepare the loss metric\n",
    "content_layer, style_layers = get_layers(content_image, style_image, generated_image)\n",
    "\n",
    "# Define loss and gradient\n",
    "loss = total_loss(content_layer, style_layers, generated_image)\n",
    "grads = K.gradients(loss, generated_image)\n",
    "\n",
    "# Define the output\n",
    "outputs = [loss]\n",
    "outputs += grads\n",
    "f_outputs = K.function([generated_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking pretty good! We almost got everything in order now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Evaluator\n",
    "To optimize, we'll be using an algorithm called L-BFGS which is a little bit better for this task than just simple gradient descent. The problem is that we'll be using this from scipy, and it expects the data in a format that isn't how Keras naturally handles the gradients and loss. To rectify this, we construct an Evaluator class that stores this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    \"\"\"\n",
    "    Evaluator class used to track gradients and loss values together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "def eval_loss_and_grads(generated):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradients\n",
    "    :param generated: The generated image\n",
    "    :return: The loss and the gradients\n",
    "    \"\"\"\n",
    "    generated = generated.reshape((1, img_height, img_width, 3))\n",
    "    outs = f_outputs([generated])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also notice that we wrote a function that returns the values as a tuple like is expected. This makes it so our evaluator class works like intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Images\n",
    "For actually saving output, here's a quick function that let's us save our images that we generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(filename, generated):\n",
    "    \"\"\"\n",
    "    Saves the generated image\n",
    "    :param filename: The filename that the image is saved to\n",
    "    :param generated: The image that we want saved\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    # Reshape image and flip from BGR to RGB\n",
    "    generated = generated.reshape((img_height, img_width, 3))\n",
    "    generated = generated[:, :, ::-1]\n",
    "\n",
    "    # Re-apply the mean shift\n",
    "    generated[:, :, 0] += 103.939\n",
    "    generated[:, :, 1] += 116.779\n",
    "    generated[:, :, 2] += 123.68\n",
    "\n",
    "    # Clip values to 0-255\n",
    "    generated = np.clip(generated, 0, 255).astype('uint8')\n",
    "\n",
    "    imsave(filename, Image.fromarray(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Optimization Loop\n",
    "Time to put it all together and finish up our code. Here's our last little snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "iterations = args.iter\n",
    "\n",
    "name = '{}-{}{}'.format(target_path, 0, target_extension)\n",
    "save_image(name, generated_img)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Iteration:', i)\n",
    "    start_time = time.time()\n",
    "    generated_img, min_val, info = fmin_l_bfgs_b(evaluator.loss, generated_img.flatten(),\n",
    "                                                 fprime=evaluator.grads, maxfun=20)\n",
    "    print('Loss:', min_val)\n",
    "    end_time = time.time()\n",
    "    print('Iteration {} took {} seconds'.format(i, end_time - start_time))\n",
    "    name = '{}-{}{}'.format(target_path, i+1, target_extension)\n",
    "    save_image(name, generated_img)\n",
    "    print('Saved image to: {}'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We construct our evaluator and grab our iteration count\n",
    "- We save the random noise as a baseline for what our generated image starts as\n",
    "- We loop through our iterations, timing as we go\n",
    "    - First we use our optimization algorithm to otpimize \n",
    "    - Then we save our image\n",
    "\n",
    "By default, this runs about 10 times and seems to work pretty well. And that's all there is to it! By running this, you should have a pretty fantastic script that you can play with, modify, and create your own style transfers!\n",
    "\n",
    "Here is the entire script put together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "style-transfer.py - An implementation of the style transfer algorithm. It's a synthesis of the original paper, combined\n",
    "                    with the adaption to the loss function that adds in the variation loss factor for normalization.\n",
    "                    Components have been synthesized together.\n",
    "\n",
    "For reference:\n",
    "    - https://arxiv.org/pdf/1508.06576.pdf (original style loss paper)\n",
    "    - https://arxiv.org/pdf/1412.0035.pdf (explains the ideas behind variation loss)\n",
    "    - https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py\n",
    "      (style transfer as given by the keras team)\n",
    "    - https://harishnarayanan.org/writing/artistic-style-transfer/ (longer tutorial that walks through convolutions)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Image neural style transfer implemented with Keras')\n",
    "parser.add_argument('content_img', metavar='content', type=str, help='Path to target content image')\n",
    "parser.add_argument('style_img', metavar='style', type=str, help='Path to target style image')\n",
    "parser.add_argument('result_img_prefix', metavar='res_prefix', type=str, help='Name of generated image')\n",
    "parser.add_argument('--iter', type=int, default=10, required=False, help='Number of iterations to run')\n",
    "parser.add_argument('--content_weight', type=float, default=0.025, required=False, help='Content weight')\n",
    "parser.add_argument('--style_weight', type=float, default=1.0, required=False, help='Style weight')\n",
    "parser.add_argument('--var_weight', type=float, default=1.0, required=False, help='Total Variation weight')\n",
    "parser.add_argument('--height', type=int, default=512, required=False, help='Height of the images')\n",
    "parser.add_argument('--width', type=int, default=512, required=False, help='Width of the images')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Params #\n",
    "\n",
    "img_height = args.height\n",
    "img_width = args.width\n",
    "img_size = img_height * img_width\n",
    "img_channels = 3\n",
    "\n",
    "content_path = args.content_img\n",
    "style_path = args.style_img\n",
    "target_path = args.result_img_prefix\n",
    "target_extension = '.png'\n",
    "\n",
    "CONTENT_IMAGE_POS = 0\n",
    "STYLE_IMAGE_POS = 1\n",
    "GENERATED_IMAGE_POS = 2\n",
    "\n",
    "# Params #\n",
    "\n",
    "\n",
    "def process_img(path):\n",
    "    \"\"\"\n",
    "    Function for processing images to the format we need\n",
    "    :param path: The path to the image\n",
    "    :return: The image as a data array, scaled and reflected\n",
    "    \"\"\"\n",
    "    # Open image and resize it\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((img_width, img_height))\n",
    "\n",
    "    # Convert image to data array\n",
    "    data = np.asarray(img, dtype='float32')\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "    data = data[:, :, :, :3]\n",
    "\n",
    "    # Apply pre-process to match VGG16 we are using\n",
    "    data[:, :, :, 0] -= 103.939\n",
    "    data[:, :, :, 1] -= 116.779\n",
    "    data[:, :, :, 2] -= 123.68\n",
    "\n",
    "    # Flip from RGB to BGR\n",
    "    data = data[:, :, :, ::-1]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_layers(content_matrix, style_matrix, generated_matrix):\n",
    "    \"\"\"\n",
    "    Returns the content and style layers we need for the transfer\n",
    "    :param content_matrix: The feature matrix of the content image\n",
    "    :param style_matrix:  The feature matrix of the style image\n",
    "    :param generated_matrix:  The feature matrix of the generated image\n",
    "    :return: A tuple of content layers and style layers\n",
    "    \"\"\"\n",
    "    # Prep the model for our new input sizes\n",
    "    input_tensor = K.concatenate([content_matrix, style_matrix, generated_matrix], axis=0)\n",
    "    model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "\n",
    "    # Convert layers to dictionary\n",
    "    layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "    # Pull the specific layers we want\n",
    "    c_layers = layers['block2_conv2']\n",
    "    s_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "    s_layers = [layers[layer] for layer in s_layers]\n",
    "\n",
    "    return c_layers, s_layers\n",
    "\n",
    "\n",
    "def content_loss(content_features, generated_features):\n",
    "    \"\"\"\n",
    "    Computes the content loss\n",
    "    :param content_features: The features of the content image\n",
    "    :param generated_features: The features of the generated image\n",
    "    :return: The content loss\n",
    "    \"\"\"\n",
    "    return 0.5 * K.sum(K.square(generated_features - content_features))\n",
    "\n",
    "\n",
    "def gram_matrix(features):\n",
    "    \"\"\"\n",
    "    Calculates the gram matrix of the feature representation matrix\n",
    "    :param features: The feature matrix that is used to calculate the gram matrix\n",
    "    :return: The gram matrix\n",
    "    \"\"\"\n",
    "    return K.dot(features, K.transpose(features))\n",
    "\n",
    "\n",
    "def style_loss(style_matrix, generated_matrix):\n",
    "    \"\"\"\n",
    "    Computes the style loss of the transfer\n",
    "    :param style_matrix: The style representation from the target style image\n",
    "    :param generated_matrix: The style representation from the generated image\n",
    "    :return: The loss from the style content\n",
    "    \"\"\"\n",
    "    # Permute the matrix to calculate proper covariance\n",
    "    style_features = K.batch_flatten(K.permute_dimensions(style_matrix, (2, 0, 1)))\n",
    "    generated_features = K.batch_flatten(K.permute_dimensions(generated_matrix, (2, 0, 1)))\n",
    "\n",
    "    # Get the gram matrices\n",
    "    style_mat = gram_matrix(style_features)\n",
    "    generated_mat = gram_matrix(generated_features)\n",
    "\n",
    "    return K.sum(K.square(style_mat - generated_mat)) / (4.0 * (img_channels ** 2) * (img_size ** 2))\n",
    "\n",
    "\n",
    "def variation_loss(generated_matrix):\n",
    "    \"\"\"\n",
    "    Computes the variation loss metric (used for normalization)\n",
    "    :param generated_matrix: The generated matrix\n",
    "    :return: The variation loss term for normalization\n",
    "    \"\"\"\n",
    "    a = K.square(generated_matrix[:, :img_height-1, :img_width-1, :] - generated_matrix[:, 1:, :img_width-1, :])\n",
    "    b = K.square(generated_matrix[:, :img_height-1, :img_width-1, :] - generated_matrix[:, :img_height-1, 1:, :])\n",
    "\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "\n",
    "def total_loss(c_layer, s_layers, generated):\n",
    "    \"\"\"\n",
    "    Computes the total loss of a given iteration\n",
    "    :param c_layer: The layer used to compute the content loss\n",
    "    :param s_layers: The layer(s) used to compute the style loss\n",
    "    :param generated: The generated image\n",
    "    :return: The total loss\n",
    "    \"\"\"\n",
    "\n",
    "    content_weight = args.content_weight\n",
    "    style_weight = args.style_weight\n",
    "    variation_weight = args.var_weight\n",
    "\n",
    "    # Content loss\n",
    "    content_features = c_layer[CONTENT_IMAGE_POS, :, :, :]\n",
    "    generated_features = c_layer[GENERATED_IMAGE_POS, :, :, :]\n",
    "    c_loss = content_loss(content_features, generated_features)\n",
    "\n",
    "    # Style loss\n",
    "    s_loss = None\n",
    "    for layer in s_layers:\n",
    "        style_features = layer[STYLE_IMAGE_POS, :, :, :]\n",
    "        generated_features = layer[GENERATED_IMAGE_POS, :, :, :]\n",
    "        if s_loss is None:\n",
    "            s_loss = style_loss(style_features, generated_features) * (style_weight / len(s_layers))\n",
    "        else:\n",
    "            s_loss += style_loss(style_features, generated_features) * (style_weight / len(s_layers))\n",
    "\n",
    "    # Variation loss (for regularization)\n",
    "    v_loss = variation_loss(generated)\n",
    "\n",
    "    return content_weight * c_loss + s_loss + variation_weight * v_loss\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(generated):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradients\n",
    "    :param generated: The generated image\n",
    "    :return: The loss and the gradients\n",
    "    \"\"\"\n",
    "    generated = generated.reshape((1, img_height, img_width, 3))\n",
    "    outs = f_outputs([generated])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "\n",
    "def save_image(filename, generated):\n",
    "    \"\"\"\n",
    "    Saves the generated image\n",
    "    :param filename: The filename that the image is saved to\n",
    "    :param generated: The image that we want saved\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    # Reshape image and flip from BGR to RGB\n",
    "    generated = generated.reshape((img_height, img_width, 3))\n",
    "    generated = generated[:, :, ::-1]\n",
    "\n",
    "    # Re-apply the mean shift\n",
    "    generated[:, :, 0] += 103.939\n",
    "    generated[:, :, 1] += 116.779\n",
    "    generated[:, :, 2] += 123.68\n",
    "\n",
    "    # Clip values to 0-255\n",
    "    generated = np.clip(generated, 0, 255).astype('uint8')\n",
    "\n",
    "    imsave(filename, Image.fromarray(generated))\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"\n",
    "    Evaluator class used to track gradients and loss values together\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prepare the generated image\n",
    "    generated_img = np.random.uniform(0, 255, (1, img_height, img_width, 3)) - 128.\n",
    "\n",
    "    # Load the respective content and style images\n",
    "    content = process_img(content_path)\n",
    "    style = process_img(style_path)\n",
    "\n",
    "    # Prepare the variables for the flow graph\n",
    "    content_image = K.variable(content)\n",
    "    style_image = K.variable(style)\n",
    "    generated_image = K.placeholder((1, img_height, img_width, 3))\n",
    "    loss = K.variable(0.)\n",
    "\n",
    "    # Grab the layers needed to prepare the loss metric\n",
    "    content_layer, style_layers = get_layers(content_image, style_image, generated_image)\n",
    "\n",
    "    # Define loss and gradient\n",
    "    loss = total_loss(content_layer, style_layers, generated_image)\n",
    "    grads = K.gradients(loss, generated_image)\n",
    "\n",
    "    # Define the output\n",
    "    outputs = [loss]\n",
    "    outputs += grads\n",
    "    f_outputs = K.function([generated_image], outputs)\n",
    "\n",
    "    evaluator = Evaluator()\n",
    "    iterations = args.iter\n",
    "\n",
    "    name = '{}-{}{}'.format(target_path, 0, target_extension)\n",
    "    save_image(name, generated_img)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print('Iteration:', i)\n",
    "        start_time = time.time()\n",
    "        generated_img, min_val, info = fmin_l_bfgs_b(evaluator.loss, generated_img.flatten(),\n",
    "                                                     fprime=evaluator.grads, maxfun=20)\n",
    "        print('Loss:', min_val)\n",
    "        end_time = time.time()\n",
    "        print('Iteration {} took {} seconds'.format(i, end_time - start_time))\n",
    "        name = '{}-{}{}'.format(target_path, i+1, target_extension)\n",
    "        save_image(name, generated_img)\n",
    "        print('Saved image to: {}'.format(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
